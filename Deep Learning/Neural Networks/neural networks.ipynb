{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fb374666",
      "metadata": {
        "id": "fb374666"
      },
      "source": [
        "<center>\n",
        "In God We Trust\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0370e29f",
      "metadata": {
        "id": "0370e29f"
      },
      "source": [
        "# CE417: Artificial Intelligence\n",
        "\n",
        "Dr. Mahdiyeh Soleymani Baghshah, Associate Professor\n",
        "\n",
        "Computer Engineering Department,\n",
        "Sharif University of Technology,\n",
        "Tehran, Tehran, Iran\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74463d06-3320-48b6-b189-c1eb3819ef06",
      "metadata": {
        "id": "74463d06-3320-48b6-b189-c1eb3819ef06"
      },
      "source": [
        "# MLP MNIST CLassifier (20 Points)\n",
        "\n",
        "Corresponding TA: Parham Saremi, Aryan Ahadinia\n",
        "\n",
        "In this question we aim to implement dense neural network from base and train a model for MNIST classification with that. MNIST is a set of 28 by 28 pixels images of handwritten digits. In this problem, you are going to implement neural network using NumPy. You are NOT PERMITTED to use any libraries except NumPy.\n",
        "\n",
        "**Required features of the model**:\n",
        "Your implementation should be parametrized and dynamic meaning that your MLP must be instantiated with any number of layers and dimension size for the layers. \n",
        "\n",
        "**NOTE**: Most of your score is for your implementation and the existence and the quality of your results (Final numbers doesn't matter that much but your model's ability to learn is important).\n",
        "\n",
        "**NOTE**: your module's logic must be implemented in NumPy without any python's for loops (or while loops :)). However, you can use for loops for iterating on different layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f28c2e6",
      "metadata": {
        "id": "0f28c2e6"
      },
      "outputs": [],
      "source": [
        "# You are denied to add any other packages.\n",
        "\n",
        "from abc import abstractmethod\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52df87d8",
      "metadata": {
        "id": "52df87d8"
      },
      "source": [
        "## Loss Function (3 Points)\n",
        "\n",
        "Loss function is one of the most important part of most of the ML methods. In this part, we want to implement loss function. We implemented an abstract class for loss function `LossFunction`. In following cells, you have to implement Mean Squared Error, Mean Absolute Error and Cross Entropy Loss. You have to implement `forward` and `backward` methods.\n",
        "\n",
        "Hint: You must save some variables as a field in the class instance in `forward` call. You are going to need them in ‍`backward` call to calculate gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a553f79d",
      "metadata": {
        "id": "a553f79d"
      },
      "outputs": [],
      "source": [
        "class LossFunction:\n",
        "    @abstractmethod\n",
        "    def forward(self, y_hat, y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self):\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5d6405e3",
      "metadata": {
        "id": "5d6405e3"
      },
      "outputs": [],
      "source": [
        "class MeanSquaredError(LossFunction):\n",
        "    def forward(self, y_hat, y):\n",
        "        # Hint: Saving some fields for backward\n",
        "        self.m = y.shape[1]\n",
        "        self.y_hat = y_hat\n",
        "        self.y = y\n",
        "        # sum((self.y_hat - self.y) ^ 2)/self.m\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return np.mean(np.square(y_hat - y))\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return 2 * (self.y_hat - self.y) / self.m\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Mean Squared Error\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fda0f3b1",
      "metadata": {
        "id": "fda0f3b1"
      },
      "outputs": [],
      "source": [
        "class MeanAbsoluteError(LossFunction):\n",
        "    def forward(self, y_hat, y):\n",
        "        self.m = y.shape[1]\n",
        "        self.y_hat = y_hat\n",
        "        self.y = y\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return np.mean(np.abs(y_hat - y))\n",
        "\n",
        "    def backward(self):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return (self.y_hat - self.y) / self.m\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Mean Absolute Error\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "38afe418",
      "metadata": {
        "id": "38afe418"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss(LossFunction):\n",
        "    def forward(self, y_hat, y):\n",
        "        self.m = y.shape[1]\n",
        "        self.y_hat = y_hat\n",
        "        self.y = y\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
        "\n",
        "    def backward(self):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return (self.y_hat - self.y) / (self.y_hat * (1 - self.y_hat) * self.m)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Cross Entropy Loss\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab75c2f",
      "metadata": {
        "id": "5ab75c2f"
      },
      "source": [
        "## Activation Functions (3 Points)\n",
        "\n",
        "Now we are going to implement some activation functions. We will implement the following activation functions: Sigmoid, Leaky ReLU, and Softmax. You have to implement both forward and backward methods for this class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "50305a98",
      "metadata": {
        "id": "50305a98"
      },
      "outputs": [],
      "source": [
        "class ActivationFunction:\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self):\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "abbed3c8",
      "metadata": {
        "id": "abbed3c8"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(ActivationFunction):\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return self.forward(self.x) * (1 - self.forward(self.x))\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Sigmoid\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a1d9f11f",
      "metadata": {
        "id": "a1d9f11f"
      },
      "outputs": [],
      "source": [
        "class LeakyReLU(ActivationFunction):\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return np.maximum(x, self.alpha * x)\n",
        "\n",
        "    def backward(self):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return np.where(self.x > 0, 1, self.alpha)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Leaky ReLU\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ca98ec03",
      "metadata": {
        "id": "ca98ec03"
      },
      "outputs": [],
      "source": [
        "class Softmax(ActivationFunction):\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        exp = np.exp(x)\n",
        "        return exp / np.sum(exp, axis=0)\n",
        "\n",
        "    def backward(self):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        return(1 - self.forward(self.x)) * self.forward(self.x)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"Softmax\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c87086",
      "metadata": {
        "id": "c5c87086"
      },
      "source": [
        "## Dense Layer (4 Points)\n",
        "\n",
        "Now it's the time to implement a single dense layer. Each dense layer has an an input vector and output vector size and an activation function. You have to implement two methods: `forward` and `backward`.\n",
        "\n",
        "Hint: `backward` method get gradient of previous backward step as input, it has to calculate gradient of weights and biases and save them in the class instance and return gradient of this step as output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f6cecf1a",
      "metadata": {
        "id": "f6cecf1a"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        self.w = np.random.randn(output_size, input_size) * np.sqrt(2 / input_size)\n",
        "        self.b = np.zeros((output_size, 1))\n",
        "\n",
        "        # Leave these fields unchanged, Use them in forward and backward\n",
        "        self.z = None  # Output of transformation\n",
        "        self.a = None  # Output of activation\n",
        "        self.dw = None  # Gradient of weights\n",
        "        self.db = None  # Gradient of biases\n",
        "\n",
        "    def forward(self, x):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        self.z = self.w @ x + self.b\n",
        "        self.a = self.activation.forward(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, da):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        dz = da * self.activation.backward()\n",
        "        self.dw = dz @ self.a.T\n",
        "        self.db = np.sum(dz, axis=1, keepdims=True)\n",
        "        return self.w.T @ dz\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.w -= lr * self.dw\n",
        "        self.b -= lr * self.db\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Dense {self.input_size} -> {self.output_size} with {self.activation}\"\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7767b08",
      "metadata": {
        "id": "e7767b08"
      },
      "source": [
        "## Data (2 Points)\n",
        "\n",
        "In cells below, we have implemented `Data` and `DataLoader` classes. Now you have to use these classes and load MNIST dataset on them. You have to normalize value of each pixel in way that it goes to interval of [0-1] and after that, shift the data in way that it get zero mean. You have to download MNIST data and you are not permitted to use libraries to load that data. You have to create two datasets: Train and Test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9256fd93",
      "metadata": {
        "id": "9256fd93"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(zip(self.x, self.y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dafa1aa3",
      "metadata": {
        "id": "dafa1aa3"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(0, len(self.data), self.batch_size):\n",
        "            yield self.data[i : i + self.batch_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8f2d9392",
      "metadata": {
        "id": "8f2d9392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6837ccc8-8da7-443a-915c-8433525116b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "###################################################\n",
        "########## Load MNIST Dataset, CODE HERE ##########\n",
        "\n",
        "###################################################\n",
        "from keras.datasets import mnist\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZnM0G6r39yc",
        "outputId": "dfc82b79-6c67-49b3-a58f-24ae123eeef9"
      },
      "id": "gZnM0G6r39yc",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v0ws4f83_ny",
        "outputId": "9a122411-ddee-4ce8-8ac1-b81d1edcf833"
      },
      "id": "3v0ws4f83_ny",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPBwWvnc4EjM",
        "outputId": "187fddfd-1ee2-482a-e672-66f81f5ff07b"
      },
      "id": "FPBwWvnc4EjM",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3198552",
      "metadata": {
        "id": "e3198552"
      },
      "source": [
        "## Neural Network (4 Points)\n",
        "\n",
        "Now we are going to implement a neural network. You have to implement `forward`, `backward`, and `fit` functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "386c2dc9",
      "metadata": {
        "id": "386c2dc9"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, loss):\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, da):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        for layer in reversed(self.layers):\n",
        "            reversed_layer = layer.backward(reversed_layer)\n",
        "        return reversed_layer\n",
        "        \n",
        "\n",
        "    def update(self, lr):\n",
        "        for layer in self.layers:\n",
        "            layer.update(lr)\n",
        "\n",
        "    def fit(self, data, epoch_number, learningRate, batch_size=1):\n",
        "        ###################################\n",
        "        ############ CODE HERE ############\n",
        "        ###################################\n",
        "        data = DataLoader(data, batch_size)\n",
        "        for epoch in range(epoch_number):\n",
        "            for x, y in data:\n",
        "                y_hat = self.forward(x)\n",
        "                back_loss = self.loss.backward(y_hat, y)\n",
        "                self.backward(back_loss)\n",
        "                self.update(learningRate)\n",
        "            print(f\"Epoch {epoch} Loss: {self.loss.forward(self.forward(x), y)}\")\n",
        "            # with this info we can draw the loss curve that we want in the next two part\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"\\n\".join([str(layer) for layer in self.layers])\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.__repr__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c086c4a",
      "metadata": {
        "id": "5c086c4a"
      },
      "source": [
        "## Training Model (2 Points)\n",
        "\n",
        "Now, use your neural network to train a model to predict class of MNIST images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2f71ef53",
      "metadata": {
        "id": "2f71ef53"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "########## Train the Model #############\n",
        "########################################\n",
        "\n",
        "'''\n",
        "برای آموزش کافی است دو لایه با اندازه های \n",
        "128 - 10\n",
        "تعریف کنیم و همه چیز\n",
        "رو خودشون ترین میکنن\n",
        "\n",
        "\n",
        "برای تعریف لایه هم از همین \n",
        "LAyer\n",
        "که تعریف کردیم استفاده میکنیم.\n",
        "'''\n",
        "# define Data and dataloader\n",
        "data = Data(train_X, train_y)\n",
        "dl = DataLoader(data, batch_size = 32)\n",
        "# define loss function\n",
        "loss = LeakyReLU\n",
        "# define layers\n",
        "\n",
        "layer1 = (28 * 28, 128, LeakyReLU)\n",
        "layer2 = (128, 10, LeakyReLU)\n",
        "layers = [layer1, layer2]\n",
        "# define neural network\n",
        "nn = NeuralNetwork(layers, loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.fit(data, epoch_number = 2, learningRate = 0.1, batch_size=32)"
      ],
      "metadata": {
        "id": "NzcBnvuhBBKY"
      },
      "id": "NzcBnvuhBBKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1b236c25",
      "metadata": {
        "id": "1b236c25"
      },
      "source": [
        "## Loss Curve (1 Points)\n",
        "\n",
        "Plot curve of loss in each epoch. It should be an smooth descending function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4408799a",
      "metadata": {
        "id": "4408799a"
      },
      "outputs": [],
      "source": [
        "########################################\n",
        "########## Plot loss curve #############\n",
        "########################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524cc034",
      "metadata": {
        "id": "524cc034"
      },
      "source": [
        "## Evaluation (1 Points)\n",
        "\n",
        "Now evaluate your model and measure how accurate is your model on both train and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "8720070c",
      "metadata": {
        "id": "8720070c"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y):\n",
        "    ###################################\n",
        "    ############ CODE HERE ############\n",
        "    ###################################\n",
        "    return sum(y == y_hat)\n",
        "\n",
        "\n",
        "########################################\n",
        "########## Calculate accuracy ##########\n",
        "########################################\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}