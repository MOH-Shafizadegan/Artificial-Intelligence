{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0iJo4ayoNcf"
      },
      "source": [
        "<center>\n",
        "In God We Trust\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpGluo9BoNcl"
      },
      "source": [
        "# CE417: Artificial Intelligence\n",
        "\n",
        "Dr. Mahdiyeh Soleymani Baghshah, Associate Professor\n",
        "\n",
        "Computer Engineering Department,\n",
        "Sharif University of Technology,\n",
        "Tehran, Tehran, Iran\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJIyal6boNcn"
      },
      "source": [
        "# Comment Classification (25 Points)\n",
        "\n",
        "Corresponding TA: Aryan Ahadinia\n",
        "\n",
        "In online retail stores, like digikala, people can leave comments on products and share their opinion. It's important to make sure that comment don't violet regulations so these website employ some people as comment reviewer to review comments one by and accept or reject comments. In this problem, we want to develop ML models to do comment reviewing task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rNLIPWnXoNcp"
      },
      "outputs": [],
      "source": [
        "# You are denied to add any other packages.\n",
        "# !pip install -U featuretools\n",
        "# import featuretools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjc9v9cGoNcs"
      },
      "source": [
        "## Data (1 Point)\n",
        "\n",
        "We want to work on data obtained from digikala. First, load train and test data separately. Then split train data into train data and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VjyaALSvoNct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c05db352-c4d4-4eae-ffcb-2bc963c8a2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#############################################\n",
        "#### Load Train and Test Data, CODE HERE ####\n",
        "#############################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_data = pd.read_csv('/content/drive/My Drive/train.csv')\n",
        "test_data = pd.read_csv('/content/drive/My Drive/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rznpVblJoNcu"
      },
      "outputs": [],
      "source": [
        "####################################################\n",
        "#### Split into train and validation, CODE HERE ####\n",
        "####################################################\n",
        "train_only, verification_only = train_data.head(8000), train_data.tail(80000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_ubx9Qi1oNcv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "de392461-4daf-48dd-ba5c-1ab8a719e7a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                        title  \\\n",
              "0     12022               پاور خوبی نیست   \n",
              "1     12023           حتما پیشنهاد میکنم   \n",
              "2     12025             اسپرسو ساز مباشی   \n",
              "3     12026                 تفنگ بازی ar   \n",
              "4     12027                     عالی بود   \n",
              "...     ...                          ...   \n",
              "7995  21640                        عالیه   \n",
              "7996  21641                      شن بازی   \n",
              "7997  21642  کیفیت و ارزان بودن لپ تاپHP   \n",
              "7998  21643                   خوب و قشنگ   \n",
              "7999  21645             کیفیت پایین ساخت   \n",
              "\n",
              "                                                comment   rate  \\\n",
              "0                     خیلی دیر شارژ میشه. من پس فرستادم   15.0   \n",
              "1     اول قرار بود بخاری برقی یا فن هیتر بخریم ولی چ...   92.0   \n",
              "2     یک هفته پیش خریدم. یکی از دوستانم که کافی شاپ ...   96.0   \n",
              "3     سلام . شاید تکنولوژی ar جدید و خوب باشد اما تو...   30.0   \n",
              "4                          موزها تازه و قیمتم مناسب بود    0.0   \n",
              "...                                                 ...    ...   \n",
              "7995  خیلی عالیه ضد آب چادر خوبیه جنسش عالیه تازه سه...  100.0   \n",
              "7996  به نظرم واقعا عالی است و واقعا بازی خوبی برای ...   60.0   \n",
              "7997  نسبت ب پولی ک دارین خرجش میکنین خیلی بدرد بخور...   60.0   \n",
              "7998  دارای بافت خوب و تمیز هست .\\r\\nرنگش هم پر رنگ ...   76.0   \n",
              "7999  قسمت لبه ها و کف سینی دارای رنگ پریدگی و زبری ...   35.0   \n",
              "\n",
              "      verification_status  \n",
              "0                       0  \n",
              "1                       0  \n",
              "2                       0  \n",
              "3                       0  \n",
              "4                       0  \n",
              "...                   ...  \n",
              "7995                    0  \n",
              "7996                    0  \n",
              "7997                    0  \n",
              "7998                    0  \n",
              "7999                    0  \n",
              "\n",
              "[8000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6560ae6d-3966-4286-a14e-50e55781ee5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>comment</th>\n",
              "      <th>rate</th>\n",
              "      <th>verification_status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12022</td>\n",
              "      <td>پاور خوبی نیست</td>\n",
              "      <td>خیلی دیر شارژ میشه. من پس فرستادم</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12023</td>\n",
              "      <td>حتما پیشنهاد میکنم</td>\n",
              "      <td>اول قرار بود بخاری برقی یا فن هیتر بخریم ولی چ...</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12025</td>\n",
              "      <td>اسپرسو ساز مباشی</td>\n",
              "      <td>یک هفته پیش خریدم. یکی از دوستانم که کافی شاپ ...</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12026</td>\n",
              "      <td>تفنگ بازی ar</td>\n",
              "      <td>سلام . شاید تکنولوژی ar جدید و خوب باشد اما تو...</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12027</td>\n",
              "      <td>عالی بود</td>\n",
              "      <td>موزها تازه و قیمتم مناسب بود</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>21640</td>\n",
              "      <td>عالیه</td>\n",
              "      <td>خیلی عالیه ضد آب چادر خوبیه جنسش عالیه تازه سه...</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>21641</td>\n",
              "      <td>شن بازی</td>\n",
              "      <td>به نظرم واقعا عالی است و واقعا بازی خوبی برای ...</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>21642</td>\n",
              "      <td>کیفیت و ارزان بودن لپ تاپHP</td>\n",
              "      <td>نسبت ب پولی ک دارین خرجش میکنین خیلی بدرد بخور...</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>21643</td>\n",
              "      <td>خوب و قشنگ</td>\n",
              "      <td>دارای بافت خوب و تمیز هست .\\r\\nرنگش هم پر رنگ ...</td>\n",
              "      <td>76.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>21645</td>\n",
              "      <td>کیفیت پایین ساخت</td>\n",
              "      <td>قسمت لبه ها و کف سینی دارای رنگ پریدگی و زبری ...</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6560ae6d-3966-4286-a14e-50e55781ee5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6560ae6d-3966-4286-a14e-50e55781ee5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6560ae6d-3966-4286-a14e-50e55781ee5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "###############################\n",
        "####### Show Train Data #######\n",
        "###############################\n",
        "train_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4YNLxxiJoNcy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "01784ce5-61e6-4129-e098-502e9d82f376"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id                              title  \\\n",
              "0      18053         جنس تاریخ گذشته و فاسد شده   \n",
              "1       5088                  کیفیت بسیار پایین   \n",
              "2       2264                           خوب نیست   \n",
              "3       3352                          بدنه ضعیف   \n",
              "4      10928                مناسب برای لباسشویی   \n",
              "...      ...                                ...   \n",
              "19995   4771                      عااالیییههههه   \n",
              "19996   9631                               عالی   \n",
              "19997     57                        ابکیه بیشتر   \n",
              "19998  52458        دیجی کالا هم کلاه بردار شده   \n",
              "19999  43909  به نظرمن که خیلی خوبه وتمیزمیشوره   \n",
              "\n",
              "                                                 comment  rate  \n",
              "0      جنس تاریخ گذشته بود.حدود دو هفته می گذشت از ان...   0.0  \n",
              "1      من این محصول را امروز از دیجی کالا تحویل گرفتم...  25.0  \n",
              "2      اصلا خوب نیست، فقط در این حد خوبه که گوشی خامو...  14.0  \n",
              "3                 کلا دو بار استفاده کردم بدنه مخزن شکست  64.0  \n",
              "4      فقط برای لباسشویی خوب است و در ظرفشویی بعد از ...   0.0  \n",
              "...                                                  ...   ...  \n",
              "19995                محشره این عطر تا حالا ۴تا گرفتم ازش  88.0  \n",
              "19996                     خیلی سبک و راحته.من راضی بودم.  60.0  \n",
              "19997  کاش دوتا کش برای فیکس کردن به افتابگر تعبیه می...   0.0  \n",
              "19998  هیچ شباهت با اون چیزی که توضیح دادن و سفارش دا...  40.0  \n",
              "19999                                       من راضیم ازش  60.0  \n",
              "\n",
              "[20000 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54d4a6b5-9594-4011-bcdf-086bef8cdfae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>comment</th>\n",
              "      <th>rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18053</td>\n",
              "      <td>جنس تاریخ گذشته و فاسد شده</td>\n",
              "      <td>جنس تاریخ گذشته بود.حدود دو هفته می گذشت از ان...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5088</td>\n",
              "      <td>کیفیت بسیار پایین</td>\n",
              "      <td>من این محصول را امروز از دیجی کالا تحویل گرفتم...</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2264</td>\n",
              "      <td>خوب نیست</td>\n",
              "      <td>اصلا خوب نیست، فقط در این حد خوبه که گوشی خامو...</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3352</td>\n",
              "      <td>بدنه ضعیف</td>\n",
              "      <td>کلا دو بار استفاده کردم بدنه مخزن شکست</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10928</td>\n",
              "      <td>مناسب برای لباسشویی</td>\n",
              "      <td>فقط برای لباسشویی خوب است و در ظرفشویی بعد از ...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>4771</td>\n",
              "      <td>عااالیییههههه</td>\n",
              "      <td>محشره این عطر تا حالا ۴تا گرفتم ازش</td>\n",
              "      <td>88.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>9631</td>\n",
              "      <td>عالی</td>\n",
              "      <td>خیلی سبک و راحته.من راضی بودم.</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>57</td>\n",
              "      <td>ابکیه بیشتر</td>\n",
              "      <td>کاش دوتا کش برای فیکس کردن به افتابگر تعبیه می...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>52458</td>\n",
              "      <td>دیجی کالا هم کلاه بردار شده</td>\n",
              "      <td>هیچ شباهت با اون چیزی که توضیح دادن و سفارش دا...</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>43909</td>\n",
              "      <td>به نظرمن که خیلی خوبه وتمیزمیشوره</td>\n",
              "      <td>من راضیم ازش</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54d4a6b5-9594-4011-bcdf-086bef8cdfae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54d4a6b5-9594-4011-bcdf-086bef8cdfae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54d4a6b5-9594-4011-bcdf-086bef8cdfae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "###############################\n",
        "####### Show Test Data ########\n",
        "###############################\n",
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKeqppgfoNc0"
      },
      "source": [
        "## Data Cleaning (5 Points)\n",
        "\n",
        "One of the most important steps im ML task is data cleaning. Data cleaning, generally, aim to transform data in a known domain in which is appropriate for the task. In this section, we explore some data cleaning techniques on text data. There are several libraries for text processing in persian like `hazm` and `parsi.io`. In this section, you can use these libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp1qTjojoNc1"
      },
      "source": [
        "### Normalizations (1 Point)\n",
        "\n",
        "It is possible to have multiple form for a character or a word. For example some of characters like ک or ی in persian, has different encoding. In persian, the other problem is zero-width non joiner (ZWNJ) which may cause different written form of same words.\n",
        "\n",
        "Apply a text normalization on your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hazm==0.6.0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpwBSmWPJy-a",
        "outputId": "0879b556-0ee5-4e57-eeac-555bdfa5c4ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm==0.6.0.1\n",
            "  Downloading hazm-0.6.0.1-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from nltk==3.3->hazm==0.6.0.1) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394489 sha256=4d12802e9b18d28a8124087d7ca07ce6580d9009ec9ec99ca9b69979d49bc9f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/1d/3a/0a8c14c30132b4f9ffd796efbb6746f15b3d6bcfc1055a9346\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp38-cp38-linux_x86_64.whl size=180659 sha256=de8c9fdc08ed1e97be8ebe8d42aa15e32a81e41a214d750ddd4eeebd87454b9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/d8/9f/59fd78b2b7d1e9ffcb68fb6de80c2e7c20b804c8cbc4d8fc23\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed hazm-0.6.0.1 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cagGwJ_MoNc3"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "#### Normalizations, CODE HERE ####\n",
        "###################################\n",
        "global_titles = []\n",
        "global_comments = []\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "normalizer = hazm.Normalizer()\n",
        "\n",
        "counter = 0\n",
        "for previous_title in train_only['title']:\n",
        "    if type(previous_title) == float:\n",
        "      global_titles.append(previous_title)\n",
        "      continue\n",
        "    new_title = normalizer.normalize(previous_title)\n",
        "    # train_only.at[counter, 'title'] = new_title\n",
        "    global_titles.append(new_title)\n",
        "    counter += 1\n",
        "\n",
        "counter = 0\n",
        "for previous_comment in train_only['comment']:\n",
        "    if type(previous_comment) == float:\n",
        "      global_comments.append(previous_comment)\n",
        "      continue\n",
        "    new_comment = normalizer.normalize(previous_comment)\n",
        "    # train_only.at[counter, 'comment'] = new_comment\n",
        "    global_comments.append(new_comment)\n",
        "    counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUWN2XdehDGH",
        "outputId": "c47588e2-51f6-4acc-d4c4-76758234b26e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_9psHtrhFNi",
        "outputId": "9800c5eb-a1d5-4d0a-f491-e4b0fcffca33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT10ytaKoNc4"
      },
      "source": [
        "### Stemming/Lemmatisation (2 Points)\n",
        "\n",
        "In many languages, like persian, arabic, and english, many words have a same root. Stemming and Lemmatisation are methods to transform all words is a family to a single form. In stemming we aim to find a common form for all word. It is not necessary to find the root as the common form. Moreover, it is possible to find the common form as a meaningless word. In lemmatisation we aim to find the root as the common form.\n",
        "\n",
        "Apply both Stemming and Lemmatisation on data. In following cells, you can choose to use which of the outputs at your own discretion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gydq9HlNoNc6"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "####### Stemming, CODE HERE #######\n",
        "###################################\n",
        "\n",
        "stemmer = hazm.Stemmer()\n",
        "\n",
        "\n",
        "titles = []\n",
        "counter = 0\n",
        "for previous_title in global_titles:\n",
        "    if type(previous_title) == float:\n",
        "      titles.append(previous_title)\n",
        "      continue\n",
        "    new_title = stemmer.stem(previous_title)\n",
        "    titles.append(new_title)\n",
        "    counter += 1\n",
        "\n",
        "comments = []\n",
        "counter = 0\n",
        "for previous_comment in global_comments:\n",
        "    if type(previous_comment) == float:\n",
        "      comments.append(previous_comment)\n",
        "      continue\n",
        "    new_comment = stemmer.stem(previous_comment)\n",
        "    comments.append(new_comment)\n",
        "    \n",
        "    counter += 1\n",
        "global_titles = titles\n",
        "global_comments = comments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pVoe_9WGoNc7"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "#### Lemmatisation, CODE HERE #####\n",
        "###################################\n",
        "lemmatizer = hazm.Lemmatizer()\n",
        "\n",
        "titles = []\n",
        "counter = 0\n",
        "for previous_title in global_titles:\n",
        "    if type(previous_title) == float:\n",
        "      titles.append(previous_title)\n",
        "      continue\n",
        "    new_title = lemmatizer.lemmatize(previous_title)\n",
        "    titles.append(new_title)\n",
        "    counter += 1\n",
        "\n",
        "comments = []\n",
        "counter = 0\n",
        "for previous_comment in global_comments:\n",
        "    if type(previous_comment) == float:\n",
        "      comments.append(previous_comment)\n",
        "      continue\n",
        "    new_comment = lemmatizer.lemmatize(previous_comment)\n",
        "    comments.append(new_comment)\n",
        "    counter += 1\n",
        "  \n",
        "global_titles = titles\n",
        "global_comments = comments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqXy1lSMgZBz",
        "outputId": "c587d1b8-b6f7-43be-c4fb-d086f5aa7487"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-WMFknSgcjZ",
        "outputId": "29529a35-42ca-4a5e-bd6e-552413407c1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAifpNi4oNc8"
      },
      "source": [
        "### Stop Words / Punctuations Removal (2 Points)\n",
        "\n",
        "In all languages, including Persian, there are very frequent words, including conjunctions, prepositions, and documentary verbs, which do not carry much meaning. Also, in normal natural language processing tasks, punctuation marks such as periods and commas are removed to clean the data.\n",
        "\n",
        "Remove stop words and punctuations in following cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "در ادامه ابتدا سعی کردم به کمک توابع آماده در این کتابخانه کارهای مربوطه را انجام دهم\n",
        "بدین صورت که به کمک تابع\n",
        "tagger.tag\n",
        "و قبل از آن تابع\n",
        "word_tokenize\n",
        "نقش ها را تشخیص داده و سپس\n",
        "اگر نقش هایی که سوال مدنظر آن است در آن بود حذف شوند اما در انجام این کار دچار مشکل شدم بدین گونه که\n",
        "در کولب چند ساعت وقت گذاشتم\n",
        "اما سشن کرش میکرد و کسی علت آن را نمیدانست و میگفتند در ویندوز مشکل دارد.\n",
        "در پای چارم اضافه کردن کتابخانه های داخلی به طور خودکار انجام نمیشد و نهایتا در \n",
        "وی اس کد نیز کتابخانه های مورد نیاز به درستی هندل نشد.\n",
        "\n",
        "بنابراین به کمک بخش\n",
        "stopwords\n",
        "که در گیت هاب مربوط به هضم آن را پیدا کردم این بخش را نوشتم\n",
        "\n",
        "پیاده سازی قبلی به صورت کامنت مشخص شده است\n",
        "'''"
      ],
      "metadata": {
        "id": "WSioeeishuUr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "465341d5-713e-4047-c7b5-9e7fca00fcae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nدر ادامه ابتدا سعی کردم به کمک توابع آماده در این کتابخانه کارهای مربوطه را انجام دهم\\nبدین صورت که به کمک تابع\\ntagger.tag\\nو قبل از آن تابع\\nword_tokenize\\nنقش ها را تشخیص داده و سپس\\nاگر نقش هایی که سوال مدنظر آن است در آن بود حذف شوند اما در انجام این کار دچار مشکل شدم بدین گونه که\\nدر کولب چند ساعت وقت گذاشتم\\nاما سشن کرش میکرد و کسی علت آن را نمیدانست و میگفتند در ویندوز مشکل دارد.\\nدر پای چارم اضافه کردن کتابخانه های داخلی به طور خودکار انجام نمیشد و نهایتا در \\nوی اس کد نیز کتابخانه های مورد نیاز به درستی هندل نشد.\\n\\nبنابراین به کمک بخش\\nstopwords\\nکه در گیت هاب مربوط به هضم آن را پیدا کردم این بخش را نوشتم\\n\\nپیاده سازی قبلی به صورت کامنت مشخص شده است\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip\n",
        "# !mkdir resources\n",
        "# !unzip ./resources-0.5.zip -d ./resources"
      ],
      "metadata": {
        "id": "b-67tu7IGeKt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import hazm\n",
        "# # from hazm import POSTagger\n",
        "# tagger = hazm.POSTagger(model='resources/postagger.model')"
      ],
      "metadata": {
        "id": "1rAYF7GVGu2Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  from nltk import tagger\n",
        "# tagger.tag ( hazm.word_tokenize( 'ما بسیار کتاب می‌خوانیم' ))"
      ],
      "metadata": {
        "id": "LRW1mG-fA2So"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "####### Stop Word Removal, CODE HERE #######\n",
        "############################################\n",
        "\n",
        "# all_titles = []\n",
        "# counter = 0\n",
        "# for previous_title in global_titles:\n",
        "#     if type(previous_title) == float:\n",
        "#       continue\n",
        "#     tokenized = None\n",
        "#     token_out = None\n",
        "#     print(\"*\")\n",
        "#     try:\n",
        "#       print(previous_title)\n",
        "#       token_out = word_tokenize(previous_title)\n",
        "#       print(\"*1\")\n",
        "#     except:\n",
        "#       all_titles.append(previous_title)\n",
        "#       print(\"*2\")\n",
        "#       continue\n",
        "#     if token_out == None:\n",
        "#       continue\n",
        "#     print(\"#\")\n",
        "#     try:\n",
        "#       tokenized = tagger.tag(token_out)\n",
        "#     except:\n",
        "#       all_titles.append(previous_title)\n",
        "#       continue\n",
        "#     if tokenized == None:\n",
        "#       continue\n",
        "#     print(\"%\")\n",
        "#     to_delete = []\n",
        "#     for token in tokenized:\n",
        "#       if token[1] == 'CONJ' or token[1] == 'P' or token[1] == 'V':\n",
        "#         to_delete.append(token)\n",
        "#     for to_ in to_delete:\n",
        "#       tokenized.remove(to_)\n",
        "#     all_titles.append(tokenized)\n",
        "#     counter += 1\n",
        "\n",
        "\n",
        "# counter = 0\n",
        "# all_comments = []\n",
        "# for previous_comment in global_comments:\n",
        "#     if type(previous_comment) == float:\n",
        "#       continue\n",
        "#     tokenized = None\n",
        "#     token_out = None\n",
        "#     try:\n",
        "#       token_out = word_tokenize(previous_comment)\n",
        "#     except:\n",
        "#       all_comments.append(previous_comment)\n",
        "#       continue\n",
        "    \n",
        "#     try:\n",
        "#       tokenized = tagger.tag(token_out)\n",
        "#     except:\n",
        "#       all_comments.append(previous_comment)\n",
        "#       continue\n",
        "\n",
        "#     new_ = []\n",
        "#     to_delete = []\n",
        "#     for token in tokenized:\n",
        "#       if token[1] == 'CONJ' or token[1] == 'P' or token[1] == 'V':\n",
        "#         to_delete.append(token)\n",
        "#       else:\n",
        "#         new_.append(token)\n",
        "\n",
        "#     all_comments.append(new_)\n",
        "#     counter += 1\n",
        "  \n",
        "# global_comments = all_comments\n",
        "# global_titles = all_titles"
      ],
      "metadata": {
        "id": "B6XxU_28v8Hs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = hazm.stopwords_list()"
      ],
      "metadata": {
        "id": "z0fKzwTNjLMO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK8XxrlUjfsU",
        "outputId": "3bd5bfc2-0b78-44bf-9f23-6684983b02ef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "389"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "L63QhmKaaw_V"
      },
      "outputs": [],
      "source": [
        "all_titles = [dict() for i in range(len(global_titles))]\n",
        "all_comments = [dict() for i in range(len(global_comments))]\n",
        "\n",
        "for title_counter in range(len(global_titles)):\n",
        "  previous_title = global_titles[title_counter]\n",
        "  if type(previous_title) == float:\n",
        "    continue\n",
        "  tokenized = hazm.word_tokenize(previous_title)\n",
        "\n",
        "  for token in tokenized:\n",
        "    if token not in stop_words:\n",
        "      if token in all_titles[title_counter]:\n",
        "        all_titles[title_counter][token] += 1\n",
        "      else:\n",
        "        all_titles[title_counter][token] = 1\n",
        "    \n",
        "for comment_counter in range(len(global_comments)):\n",
        "  previous_comment = global_comments[comment_counter]\n",
        "  if type(previous_comment) == float:\n",
        "    continue\n",
        "  tokenized = hazm.word_tokenize(previous_comment)\n",
        "\n",
        "  for token in tokenized:\n",
        "    if token not in stop_words:\n",
        "      if token in all_comments[comment_counter]:\n",
        "        all_comments[comment_counter][token] += 1\n",
        "      else:\n",
        "        all_comments[comment_counter][token] = 1\n",
        "\n",
        "\n",
        "global_titles = all_titles\n",
        "global_comments = all_comments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEmSJu8mWwX",
        "outputId": "c0831468-8416-4a6f-bd31-7dd0a85725c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYoqo25mmKM2",
        "outputId": "7300454c-05ef-4101-8ab8-4139f3e5d008"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "i6ERbxqZoNc-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c9007c9e-d584-4703-d450-f7dbaf8a171d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nاین بخش از کد نیز مانند بخش قبلی که توضیح داده شد، روش پیاده سازی اش تغییر کرد\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "############################################\n",
        "###### Punctuations Removal, CODE HERE #####\n",
        "############################################\n",
        "'''\n",
        "این بخش از کد نیز مانند بخش قبلی که توضیح داده شد، روش پیاده سازی اش تغییر کرد\n",
        "'''\n",
        "# all_titles = []\n",
        "# counter = 0\n",
        "# for previous_title in global_titles:\n",
        "#     to_delete = []\n",
        "#     for token in previous_title:\n",
        "#       if token[1] == 'PUNC':\n",
        "#         to_delete.append(token)\n",
        "#     for to_ in to_delete:\n",
        "#       previous_title.remove(to_)\n",
        "#     all_titles.append(previous_title)\n",
        "#     counter += 1\n",
        "\n",
        "\n",
        "# counter = 0\n",
        "# all_comments = []\n",
        "# for previous_comment in global_comments:\n",
        "#     new_ = []\n",
        "#     to_delete = []\n",
        "#     for token in previous_comment:\n",
        "#       if token[1] == 'PUNC':\n",
        "#         to_delete.append(token)\n",
        "#       else:\n",
        "#         new_.append(token)\n",
        "\n",
        "#     all_comments.append(new_)\n",
        "#     counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = [',', '!', '?', ':', '،', '؛', ',', '؟', '.']"
      ],
      "metadata": {
        "id": "AvWmceR7myf9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ak5zv3oam4aC"
      },
      "outputs": [],
      "source": [
        "all_titles = [dict() for i in range(len(global_titles))]\n",
        "all_comments = [dict() for i in range(len(global_comments))]\n",
        "\n",
        "for title_counter in range(len(global_titles)):\n",
        "  previous_title = global_titles[title_counter]\n",
        "  if type(previous_title) == float:\n",
        "    continue\n",
        "  tokenized = previous_title\n",
        "\n",
        "  for token in tokenized:\n",
        "    if token not in punctuations:\n",
        "      if token in all_titles[title_counter]:\n",
        "        all_titles[title_counter][token] += 1\n",
        "      else:\n",
        "        all_titles[title_counter][token] = 1\n",
        "    \n",
        "for comment_counter in range(len(global_comments)):\n",
        "  previous_comment = global_comments[comment_counter]\n",
        "  if type(previous_comment) == float:\n",
        "    continue\n",
        "  tokenized = previous_comment\n",
        "\n",
        "  for token in tokenized:\n",
        "    if token not in punctuations:\n",
        "      if token in all_comments[comment_counter]:\n",
        "        all_comments[comment_counter][token] += 1\n",
        "      else:\n",
        "        all_comments[comment_counter][token] = 1\n",
        "\n",
        "\n",
        "global_titles = all_titles\n",
        "global_comments = all_comments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFB1SulZoE2m",
        "outputId": "c991b653-821c-432b-f813-f8d9aa48af6d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(global_comments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZFuJOcToJsO",
        "outputId": "5ec5b740-8522-4ed9-e92c-2ff7130bf504"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = -1\n",
        "for x in global_titles:\n",
        "  c += 1\n",
        "  for y in x:\n",
        "    if x[y] > 1:\n",
        "      print(c)"
      ],
      "metadata": {
        "id": "XWjOM5bmoOYX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At6Kz0uDoNc_"
      },
      "source": [
        "## Naive Bayes (5 Points)\n",
        "\n",
        "In this part, we want to implement a naive bayes classifier with assumption of independent distribution between features, which are tokens in our problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Zm-xBCiBoNdA"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "  def __init__(self):\n",
        "    self.predictions = []\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    if self.predictions == []:\n",
        "      # self.predictions = [[0] * len(X) for i in range(2) for j in range(2)]\n",
        "      self.predictions = np.zeros((len(X[0]), 2, 2), dtype = 'int')\n",
        "  \n",
        "    print(self.predictions.shape)\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    # return\n",
        "    for index in range(len(X[0])):\n",
        "      if index % 100 == 0:\n",
        "        print(index)\n",
        "      for t in range(len(X)):\n",
        "        val = 1 if X[t][index] > 0 else 0\n",
        "        self.predictions[index][val][y[t]] += 1\n",
        "        ##################################\n",
        "        ######### YOUR CODE HERE #########\n",
        "        ##################################\n",
        "\n",
        "  def predict(self, X):\n",
        "    answer_y_0 = 1.0\n",
        "    answer_y_1 = 1.0\n",
        "    for index in range(len(X)):\n",
        "      answer_y_0 *= (self.predictions[index][X[index]][0] / (self.predictions[index][X[index]][0] + self.predictions[index][X[index - 1]][0]))\n",
        "      answer_y_1 *= (self.predictions[index][X[index]][1] / (self.predictions[index][X[index]][1] + self.predictions[index][X[index - 1]][1]))\n",
        "    if answer_y_0 >= answer_y_1:\n",
        "      return 0\n",
        "    return 1\n",
        "        ##################################\n",
        "        ######### YOUR CODE HERE #########\n",
        "        ##################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpa0LNLUoNdB"
      },
      "source": [
        "## TF-IDF Vectorization (3 Points)\n",
        "\n",
        "There are several methods for generating a vectorized representations for tokens. Consider our token is words. and we want to develop a vectorizer to vectorize words. The main problem in vectorization is to maintain as much information as possible. A good assumptions is that frequency of a work represent the importance of that word. The assumption has a trivial exception. Stop words such as 'is', 'and', 'are', and e.t.c are very frequent. TF-TDF leverage term frequency and inverse document frequency to solve this problem. Document frequency shows that how frequents is a token among all documents. As the document frequency increases, the importance of that token drops. In the other hand, if a word has high frequency in a documents, it is likely to that word play a key role in that document so the importance of that token increases.\n",
        "\n",
        "In this section, you have to implement a TF-IDF vectorizer. Search about this method in Google and implement it.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KB-aY1qGoNdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b0da67-d3e6-43cc-ff42-4e300b21640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17991\n",
            "8000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TF_IDF:\n",
        "  def __init__(self):\n",
        "    self.all_tokens = dict()\n",
        "    self.all_documents = []\n",
        "    self.term_frequencies = []\n",
        "    self.inverse_document_frequency = dict()\n",
        "    self.number_of_documents_containing_a_token = dict()\n",
        "    self.td_ift = []\n",
        "\n",
        "  def fit(self, X) -> None:\n",
        "        ##################################\n",
        "        ######### YOUR CODE HERE #########\n",
        "        ##################################\n",
        "        doc_num = len(X)\n",
        "        self.all_documents = [dict() for i in range(doc_num)]\n",
        "        self.term_frequencies = [dict() for i in range(doc_num)]\n",
        "        self.td_ift = [dict() for i in range(doc_num)]\n",
        "        # add tokens\n",
        "        for id in range(doc_num):\n",
        "          info = X[id]\n",
        "          tokens_this_doc = []\n",
        "          for token in info:\n",
        "            if token not in tokens_this_doc:\n",
        "              tokens_this_doc.append(token)\n",
        "              if token in self.number_of_documents_containing_a_token:\n",
        "                self.number_of_documents_containing_a_token[token] += 1\n",
        "              else:\n",
        "                self.number_of_documents_containing_a_token[token] = 1\n",
        "            # add tokens to all tokens\n",
        "            if token not in self.all_tokens:\n",
        "              self.all_tokens[token] = 1\n",
        "            else:\n",
        "              self.all_tokens[token] += 1\n",
        "            # add tokens to its doc\n",
        "            if token not in self.all_documents[id]:\n",
        "              self.all_documents[id][token] = 1\n",
        "            else:\n",
        "              self.all_documents[id][token] += 1\n",
        "          \n",
        "        # calculate term frequency\n",
        "        for id in range(doc_num):\n",
        "          doc_size = len(X[id])\n",
        "          for token in self.all_documents[id]:\n",
        "            number_of_this_token_in_this_doc = self.all_documents[id][token]\n",
        "            self.term_frequencies[id][token] = number_of_this_token_in_this_doc/doc_size\n",
        "        \n",
        "        # calculate number of docs containig a token\n",
        "        # for token in self.all_tokens:\n",
        "        #   for id in range(doc_num):\n",
        "        #     info = X[id]\n",
        "        #     if id in info: # id or token?!\n",
        "        #       if token in self.number_of_documents_containing_a_token:\n",
        "        #         self.number_of_documents_containing_a_token[token] = 1\n",
        "        #       else:\n",
        "        #         self.number_of_documents_containing_a_token[token] += 1\n",
        "        \n",
        "        # calculate Inverse document frequency\n",
        "        for token in self.all_tokens:\n",
        "          self.inverse_document_frequency[token] = np.log(doc_num/(self.number_of_documents_containing_a_token[token])) # +1 in makhraj?\n",
        "\n",
        "        # calculate TF-IDF\n",
        "        for id in range(doc_num):\n",
        "          dict_ = self.term_frequencies[id]\n",
        "          for token in dict_:\n",
        "            self.td_ift[id][token] = self.term_frequencies[id][token] * self.inverse_document_frequency[token]\n",
        "        # final calculations\n",
        "\n",
        "\n",
        "\n",
        "        self.final_array = np.zeros((doc_num, len(self.all_tokens)), dtype = 'float32')\n",
        "        tokens_list = list(self.all_tokens)\n",
        "        print(len(tokens_list))\n",
        "        print(doc_num)\n",
        "        for id in range(doc_num):\n",
        "          for token_i in range(len(self.all_tokens)):\n",
        "            token = tokens_list[token_i]\n",
        "            if token in self.td_ift[id]:\n",
        "              self.final_array[id][token_i] = self.td_ift[id][token]\n",
        "\n",
        "\n",
        "          \n",
        "\n",
        "  def transform(self, X) -> np.ndarray:\n",
        "        ##################################\n",
        "        ######### YOUR CODE HERE #########\n",
        "        ##################################\n",
        "        \n",
        "        return np.array(self.final_array)\n",
        "\n",
        "\n",
        "tf_idf = TF_IDF()\n",
        "tf_idf.fit(global_comments)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_array = tf_idf.transform(global_comments)"
      ],
      "metadata": {
        "id": "OPoLv_eAzX0p"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6bRP2Y6oNdE"
      },
      "source": [
        "## Logistic Regression (5 Points)\n",
        "\n",
        "In this part we want to train a logistic regression classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "r0W-The4oNdF"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def _loss(self, X, y) -> float:\n",
        "        ##########################################################\n",
        "        ######### Calculate the loss function. CODE HERE #########\n",
        "        ######### You can use logarithm to avoid underflow #######\n",
        "        ##########################################################\n",
        "        return float(np.mean(np.log(1 + np.exp(-y * self.predict(X)))))\n",
        "\n",
        "    def _gradient(self, X, y) -> np.ndarray:\n",
        "        ##########################################################\n",
        "        ######### Calculate the gradient. CODE HERE ##############\n",
        "        ##########################################################\n",
        "        return np.mean(-y * X / (1 + np.exp(y * self.predict(X))), axis=0)\n",
        "\n",
        "    def fit(self, X, y) -> None:\n",
        "        ##########################################################\n",
        "        ######### Train the model. CODE HERE #####################\n",
        "        ##########################################################\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        for counter in range(2):\n",
        "            self.w -= self._gradient(X, y) * self.lr\n",
        "            if counter % 100 == 0:\n",
        "                print(self._loss(X, y))\n",
        "\n",
        "    def predict(self, X) -> np.ndarray:\n",
        "        ##########################################################\n",
        "        ######### Predict the result. CODE HERE ##################\n",
        "        ##########################################################\n",
        "        return np.dot(X, self.w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1kCtabkoNdH"
      },
      "source": [
        "## Evaluation (3 Points)\n",
        "\n",
        "Now we want to evaluate our models on validation data. Calculate following metrics for both models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "H1QF_d1koNdI"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_real, y_predicted) -> float:\n",
        "    ##########################################################\n",
        "    ######### Calculate the accuracy. CODE HERE ##############\n",
        "    ##########################################################\n",
        "    return float(np.mean(y_real == y_predicted))\n",
        "\n",
        "\n",
        "def precision(y_real, y_predicted) -> float:\n",
        "    ##########################################################\n",
        "    ######### Calculate the precision. CODE HERE #############\n",
        "    ##########################################################\n",
        "    return float(np.mean(y_real[y_real == y_predicted]))\n",
        "\n",
        "\n",
        "def recall(y_real, y_predicted) -> float:\n",
        "    ##########################################################\n",
        "    ######### Calculate the recall. CODE HERE ################\n",
        "    ##########################################################\n",
        "    return float(np.mean(y_predicted[y_real == y_predicted]))\n",
        "\n",
        "\n",
        "def f1_score(y_real, y_predicted) -> float:\n",
        "    ##########################################################\n",
        "    ######### Calculate the f1_score. CODE HERE ##############\n",
        "    ##########################################################\n",
        "    return 2 * (precision(y_real, y_predicted) * recall(y_real, y_predicted)) / (precision(y_real, y_predicted) + recall(y_real, y_predicted))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ApNdrk63oNdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab0f3cf-b088-4032-90f9-c083fd2066c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17991, 2, 2)\n",
            "(8000, 17991)\n",
            "(8000,)\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "17991\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-1fe7a1493eb7>:29: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  answer_y_1 *= (self.predictions[index][X[index]][1] / (self.predictions[index][X[index]][1] + self.predictions[index][X[index - 1]][1]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "############################################\n",
        "######### Metrics for Naive Bayes ##########\n",
        "############################################\n",
        "\n",
        "# based on comments\n",
        "naive = NaiveBayes()\n",
        "naive.fit(main_array, train_only['verification_status'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "possible_tokens = list(tf_idf.all_tokens)\n",
        "# naive.predict(np.empty((10, 10)))\n",
        "array = np.zeros((len(possible_tokens)), dtype = 'int')\n",
        "test = ['بخرید', 'بودم', 'راضی', 'من', 'سلام']\n",
        "for element in test:\n",
        "  if element in possible_tokens:\n",
        "    array[possible_tokens.index(element)] = 1\n",
        "print(len(array))\n",
        "print(array[2])\n",
        "predicted = naive.predict(array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LisoeB0yFQFt",
        "outputId": "36f22806-d1ce-4c72-da6a-88dd0dd3bc5e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17991\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-1fe7a1493eb7>:29: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  answer_y_1 *= (self.predictions[index][X[index]][1] / (self.predictions[index][X[index]][1] + self.predictions[index][X[index - 1]][1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: \", accuracy(1, predicted))\n",
        "print(\"Precision: \", precision(np.array([1]), np.array([predicted])))\n",
        "print(\"Recall: \", recall(np.array([1]), np.array([predicted])))\n",
        "print(\"F1 Score: \", f1_score(np.array([1]), np.array([predicted])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7yTtrzeGcVV",
        "outputId": "d71b2f3c-8299-4bfe-b45e-d10be543da85"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  1.0\n",
            "Precision:  1.0\n",
            "Recall:  1.0\n",
            "F1 Score:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYcqDNd9oNdL"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "######### Metrics for Logistic Regression ##\n",
        "############################################\n",
        "lr = LogisticRegression()\n",
        "# lr.fit(main_array, train_only['verification_status'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EofZgaM6oNdN"
      },
      "source": [
        "## Evaluation on Test data (3 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADJ_X000oNdO"
      },
      "source": [
        "At the final point, you have to run your model on test data and create a csv file with two columns: `id` and `verification_status`. Then run the `CommentJudge.jar` with following command and report your metrics. you must take an screenshot and import that in the notebook.\n",
        "\n",
        "```bash\n",
        "java -jar CommentJudge.jar ans.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "pd_NzfQ-oNdP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "85b49d3b-3112-4ee3-8f4e-b7846d5ff72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Something went wrong, Probably your csv file is missing or malformed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nبه دلیل اینکه تعداد داده ها نامناسب است نمیتوان این کار را بر ری اینها انجام داد\\nدر واقع تعداد این داده ها طوری نیست که خواسته های سوال را بتوان پیاده سازی کرد\\n\\nالبته کدهای مربوط به کلاس ها زده شده اما خروجی ها اینجا نشان داده نشده است\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "###################################\n",
        "############ Code Here ############\n",
        "###################################\n",
        "! java -jar '/content/drive/My Drive/CommentJudge.jar' 'ans.csv'\n",
        "'''\n",
        "به دلیل اینکه تعداد داده ها نامناسب است نمیتوان این کار را بر ری اینها انجام داد\n",
        "در واقع تعداد این داده ها طوری نیست که خواسته های سوال را بتوان پیاده سازی کرد\n",
        "\n",
        "البته کدهای مربوط به کلاس ها زده شده اما خروجی ها اینجا نشان داده نشده است\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}